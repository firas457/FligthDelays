{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# _Predicting Flight Delays_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10415/543716832.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/home/firas/Desktop/T_ONTIME_MARKETING.csv\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Searching for the least amount of unique values in  each column to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['DUP', 'CANCELLED', 'DEP_DEL15', 'ARR_DEL15', 'FLIGHTS', 'DIV_AIRPORT_LANDINGS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can tell there are a few columns that describe almost the same thing.\n",
    "We want to drop them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['FL_DATE', 'MKT_UNIQUE_CARRIER', 'BRANDED_CODE_SHARE', 'TAIL_NUM', 'OP_CARRIER_FL_NUM',\n",
    "         'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'ARR_DELAY_GROUP',\n",
    "         'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST', 'DEST_CITY_NAME', 'DEST_STATE_ABR', 'DEST_STATE_NM', 'DEST_WAC',\n",
    "         'DEP_DELAY', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'ARR_DELAY_NEW',\n",
    "         'DISTANCE_GROUP'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking for duplicated rows, to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "No duplicated rows were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Filling NaN values with zeros in each of the delay reason columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['CARRIER_DELAY'] = df['CARRIER_DELAY'].fillna(0)\n",
    "df['WEATHER_DELAY'] = df['WEATHER_DELAY'].fillna(0)\n",
    "df['NAS_DELAY'] = df['NAS_DELAY'].fillna(0)\n",
    "df['SECURITY_DELAY'] = df['SECURITY_DELAY'].fillna(0)\n",
    "df['LATE_AIRCRAFT_DELAY'] = df['LATE_AIRCRAFT_DELAY'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We want to merge 5 columns, giving each reason a number 1-5: <br>\n",
    "- 'CARRIER_DELAY'\n",
    "- 'WEATHER_DELAY'\n",
    "- 'NAS_DELAY'\n",
    "- 'SECURITY_DELAY'\n",
    "- 'LATE_AIRCRAFT_DELAY'\n",
    "<br> into one column __'DELAY_REASON'__. <br>\n",
    "0 defines the flights that didn't get delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conditions = [(df['CARRIER_DELAY'] != 0), (df['WEATHER_DELAY'] != 0), (df['NAS_DELAY'] != 0), (df['SECURITY_DELAY'] != 0), (df['LATE_AIRCRAFT_DELAY']) != 0]\n",
    "\n",
    "values = [1, 2, 3, 4, 5]\n",
    "df['DELAY_REASON'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['SCH_OP_UNIQUE_CARRIER','SCH_OP_CARRIER_AIRLINE_ID','SCH_OP_CARRIER','SCH_OP_CARRIER_FL_NUM'\t\n",
    "        ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dropping the columns:\n",
    "- 'CARRIER_DELAY'\n",
    "- 'WEATHER_DELAY'\n",
    "- 'NAS_DELAY'\n",
    "- 'SECURITY_DELAY'\n",
    "- 'LATE_AIRCRAFT_DELAY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Checking if the columns were dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['DIV1_AIRPORT_ID','DIV1_AIRPORT','DIV_DISTANCE','DIV_ARR_DELAY','DIV_ACTUAL_ELAPSED_TIME','DIV_REACHED_DEST','LONGEST_ADD_GTIME'\n",
    ",'TOTAL_ADD_GTIME','FIRST_DEP_TIME'\n",
    "        ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prepare the label 'ARR_DELAY' we want to predict.\n",
    "- Less than 15 minutes: 0\n",
    "- 15 minutes to 30 minutes: 1\n",
    "- 30 minutes to 45 minutes: 2\n",
    "- 45 minutes to 60 minutes: 3\n",
    "- more than 60 minutes: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "delay = []\n",
    "for flight_del in df['ARR_DELAY']:\n",
    "    if flight_del > 60:\n",
    "        delay.append(4)\n",
    "    elif flight_del > 45:\n",
    "        delay.append(3)\n",
    "    elif flight_del > 30:\n",
    "        delay.append(2)\n",
    "    elif flight_del > 15:\n",
    "        delay.append(1)\n",
    "    else:\n",
    "        delay.append(0)\n",
    "df['ARR_DELAY'] = delay\n",
    "df.rename(columns={'ARR_DELAY': 'DELAY'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert MKT_CARRIER rows : string -> number\n",
    "#| \n",
    "x=df['MKT_CARRIER'].value_counts()\n",
    "item_type_mapping={}\n",
    "item_list=x.index\n",
    "for i in range(0,len(item_list)):\n",
    "    item_type_mapping[item_list[i]]=i\n",
    "\n",
    "df['MKT_CARRIER']=df['MKT_CARRIER'].map(lambda x:item_type_mapping[x]) \n",
    "\n",
    "\n",
    "\n",
    "#convert OP_UNIQUE_CARRIER : string -> number\n",
    "\n",
    "x=df['OP_UNIQUE_CARRIER'].value_counts()\n",
    "item_type_mapping={}\n",
    "item_list=x.index\n",
    "for i in range(0,len(item_list)):\n",
    "    item_type_mapping[item_list[i]]=i\n",
    "\n",
    "df['OP_UNIQUE_CARRIER']=df['OP_UNIQUE_CARRIER'].map(lambda x:item_type_mapping[x]) \n",
    "\n",
    "\n",
    "#convert OP_CARRIER : string -> number\n",
    "\n",
    "x=df['OP_CARRIER'].value_counts()\n",
    "item_type_mapping={}\n",
    "item_list=x.index\n",
    "for i in range(0,len(item_list)):\n",
    "    item_type_mapping[item_list[i]]=i\n",
    "\n",
    "df['OP_CARRIER']=df['OP_CARRIER'].map(lambda x:item_type_mapping[x]) \n",
    "\n",
    "\n",
    "#convert ARR_TIME_BLK : string -> number\n",
    "\n",
    "x=df['ARR_TIME_BLK'].value_counts()\n",
    "item_type_mapping={}\n",
    "item_list=x.index\n",
    "for i in range(0,len(item_list)):\n",
    "    item_type_mapping[item_list[i]]=i\n",
    "\n",
    "df['ARR_TIME_BLK']=df['ARR_TIME_BLK'].map(lambda x:item_type_mapping[x]) \n",
    "\n",
    "\n",
    "#convert CANCELLATION_CODE: string -> number\n",
    "\n",
    "x=df['CANCELLATION_CODE'].value_counts()\n",
    "item_type_mapping={}\n",
    "item_list=x.index\n",
    "for i in range(0,len(item_list)):\n",
    "    item_type_mapping[item_list[i]]=i\n",
    "\n",
    "df['CANCELLATION_CODE']=df['CANCELLATION_CODE'].map(lambda x:item_type_mapping[x]) \n",
    "\n",
    "#|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " # import numpy as np\n",
    " # from keras.models import Sequential\n",
    " # from keras.layers import Dense\n",
    " # from keras.wrappers.scikit_learn import KerasClassifier\n",
    " # from sklearn.model_selection import cross_val_score\n",
    " # from sklearn.model_selection import KFold\n",
    " # from sklearn.preprocessing import MinMaxScaler\n",
    " # import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Random seed for reproducibility\n",
    " # seed = 10\n",
    " # np.random.seed(seed)\n",
    "\n",
    "\n",
    " # Y=df[\"DELAY_REASON\"]\n",
    "\n",
    " # X=df.drop('DELAY_REASON',axis=1)\n",
    "\n",
    " # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    " # X = scaler.fit_transform(X)\n",
    "\n",
    " # X = pd.DataFrame(X)\n",
    "\n",
    " # Y = pd.get_dummies(Y)\n",
    "\n",
    " # X = X.values\n",
    " # Y = Y.values\n",
    "\n",
    "# First define baseline model. Then use it in Keras Classifier for the training\n",
    " # def baseline_model():\n",
    "    # Create model here\n",
    "    #  model = Sequential()\n",
    "    #  model.add(Dense(10, input_dim = 29, activation = 'relu')) # Rectified Linear Unit Activation Function\n",
    "    #  model.add(Dense(10, activation = 'relu'))\n",
    "    #  model.add(Dense(6, activation = 'softmax')) # Softmax for multi-class classification\n",
    "    # Compile model here\n",
    "    #  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    #  return model\n",
    "\n",
    "# Create Keras Classifier and use predefined baseline model\n",
    " # estimator = KerasClassifier(build_fn = baseline_model, epochs = 100, batch_size = 10, verbose = 0)\n",
    "\n",
    "# KFold Cross Validation\n",
    " # kfold = KFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "\n",
    "\n",
    " # Object to describe the result\n",
    " # results = cross_val_score(estimator, X, Y, cv = kfold)\n",
    "# Result\n",
    " # print(\"Result: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('file1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "\n",
    "# We're using pandas to read the CSV file. This is easy for small datasets, but for large and complex datasets,\n",
    "# tensorflow parsing and processing functions are more powerful.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The CSV file does not have a header, so we have to fill in column names.\n",
    "names = [\n",
    "    'DAY_OF_MONTH', \n",
    "    'carrier DAY_OF_WEEK',\t'MKT_CARRIER_AIRLINE_ID' ,   'MKT_CARRIER'\t,'MKT_CARRIER_FL_NUM'\t,'OP_UNIQUE_CARRIER',\t'OP_CARRIER_AIRLINE_ID' ,\t'OP_CARRIER'\t\n",
    "'ORIGIN_AIRPORT_ID'\t, 'ORIGIN_STATE_FIPS'\t,'ARR_TIME'\t,'DELAY' ,\t'ARR_TIME_BLK' ,\t'CANCELLATION_CODE'\n",
    "'DIVERTED',\t'CRS_ELAPSED_TIME',\t'ACTUAL_ELAPSED_TIME',\t'AIR_TIME',\t'DISTANCE','DELAY_REASON',\n",
    "'DEST_AIRPORT_ID' , 'DEST_STATE_FIPS' ,'CRS_DEP_TIME', 'DEP_TIME' ,'DEP_DELAY_NEW','TAXI_OUT' ,'WHEELS_OFF', 'WHEELS_ON', 'CRS_ARR_TIME' \n",
    "]\n",
    " \n",
    "\n",
    "# We also have to specify dtypes.\n",
    "dtypes = {\n",
    "       'DAY_OF_MONTH': np.float32, \n",
    "    'DAY_OF_WEEK': np.float32,\t\n",
    "    'MKT_CARRIER_AIRLINE_ID' : np.float32,   \n",
    "    'MKT_CARRIER'\t: np.float32\n",
    "    ,'MKT_CARRIER_FL_NUM': np.float32\n",
    "        \t,'OP_UNIQUE_CARRIER': np.float32,\t\n",
    "    'OP_CARRIER_AIRLINE_ID': np.float32 ,\t\n",
    "    'OP_CARRIER': np.float32,\t\n",
    "'ORIGIN_AIRPORT_ID': np.float32\t,\n",
    " 'ORIGIN_STATE_FIPS': np.float32\t,\n",
    " 'ARR_TIME': np.float32\t,\t\n",
    " 'ARR_TIME_BLK': np.float32 ,\t\n",
    "'CANCELLATION_CODE': np.float32,\n",
    "'DIVERTED': np.float32,\t\n",
    "'CRS_ELAPSED_TIME': np.float32,\n",
    "\t'ACTUAL_ELAPSED_TIME': np.float32,\n",
    "       \t'AIR_TIME': np.float32,\n",
    "             \t'DISTANCE': np.float32,\n",
    "               'DELAY_REASON': np.float32,\n",
    "'DEST_AIRPORT_ID': np.float32 ,\n",
    " 'DEST_STATE_FIPS': np.float32 ,\n",
    " 'CRS_DEP_TIME': np.float32,\n",
    "   'DEP_TIME': np.float32 ,\n",
    "   'DEP_DELAY_NEW': np.float32,\n",
    "   'TAXI_OUT' : np.float32\n",
    ",'WHEELS_OFF': np.float32,\n",
    " 'WHEELS_ON': np.float32,\n",
    "   'CRS_ARR_TIME': np.float32 \n",
    "}\n",
    "\n",
    "\n",
    "df2=df.copy()\n",
    "\n",
    "# Split the data into a training set and an eval set.\n",
    "training_data = df2[:30]\n",
    "eval_data = df2[30:]\n",
    "test_data = df2[:10]\n",
    "\n",
    "# Separate input features from labels\n",
    "training_label = training_data.pop(\"DELAY\")\n",
    "eval_label = eval_data.pop(\"DELAY\")\n",
    "test_label = test_data.pop(\"DELAY\")\n",
    "\n",
    "\n",
    "# Now we can start using some TensorFlow.\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "\n",
    "training_input_fn = tf.estimator.inputs.pandas_input_fn(x=training_data, y=training_label, batch_size=64, shuffle=True, num_epochs=None)\n",
    "\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.pandas_input_fn(x=eval_data, y=eval_label, batch_size=64, shuffle=False)\n",
    "\n",
    "#test / predict\n",
    "#   shuffle=False -> do not randomize input data\n",
    "\n",
    "\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.pandas_input_fn(x=test_data, y=test_label, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "# Describe how the model should interpret the inputs. The names of the feature columns have to match the names\n",
    "# of the series in the dataframe.\n",
    "\n",
    "\n",
    "DAY_OF_MONTH1= tf.feature_column.numeric_column('DAY_OF_MONTH')\n",
    "DAY_OF_WEEK = tf.feature_column.numeric_column('DAY_OF_WEEK')\n",
    "MKT_CARRIER_AIRLINE_ID = tf.feature_column.numeric_column('MKT_CARRIER_AIRLINE_ID')\n",
    "MKT_CARRIER= tf.feature_column.numeric_column( 'MKT_CARRIER')\n",
    "MKT_CARRIER_FL_NUM = tf.feature_column.numeric_column('MKT_CARRIER_FL_NUM')\n",
    "OP_UNIQUE_CARRIER = tf.feature_column.numeric_column('OP_UNIQUE_CARRIER')\n",
    "OP_CARRIER_AIRLINE_ID= tf.feature_column.numeric_column('OP_CARRIER_AIRLINE_ID')\n",
    "OP_CARRIER = tf.feature_column.numeric_column('OP_CARRIER')\n",
    "ORIGIN_AIRPORT_ID = tf.feature_column.numeric_column('ORIGIN_AIRPORT_ID')\n",
    "ORIGIN_STATE_FIPS = tf.feature_column.numeric_column('ORIGIN_STATE_FIPS')\n",
    "ARR_TIME = tf.feature_column.numeric_column('ARR_TIME')\n",
    "ARR_TIME_BLK = tf.feature_column.numeric_column('ARR_TIME_BLK')\n",
    "CANCELLATION_CODE = tf.feature_column.numeric_column('CANCELLATION_CODE')\n",
    "DIVERTED= tf.feature_column.numeric_column('DIVERTED')\n",
    "CRS_ELAPSED_TIME = tf.feature_column.numeric_column('CRS_ELAPSED_TIME')\n",
    "ACTUAL_ELAPSED_TIME = tf.feature_column.numeric_column('ACTUAL_ELAPSED_TIME')\n",
    "AIR_TIME = tf.feature_column.numeric_column('AIR_TIME')\n",
    "DISTANCE = tf.feature_column.numeric_column('DISTANCE')\n",
    "DEST_AIRPORT_ID = tf.feature_column.numeric_column('DEST_AIRPORT_ID')\n",
    "DEST_STATE_FIPS = tf.feature_column.numeric_column('DEST_STATE_FIPS')\n",
    "CRS_DEP_TIME = tf.feature_column.numeric_column('CRS_DEP_TIME')\n",
    "DEP_TIME= tf.feature_column.numeric_column('DEP_TIME')\n",
    "DEP_DELAY_NEW = tf.feature_column.numeric_column('DEP_DELAY_NEW')\n",
    "TAXI_OUT = tf.feature_column.numeric_column('TAXI_OUT')\n",
    "WHEELS_OFF = tf.feature_column.numeric_column('WHEELS_OFF')\n",
    "WHEELS_ON = tf.feature_column.numeric_column('WHEELS_ON')\n",
    "CRS_ARR_TIME = tf.feature_column.numeric_column('CRS_ARR_TIME')\n",
    "\n",
    "\n",
    "linear_features = [DAY_OF_MONTH1, CRS_ARR_TIME, WHEELS_ON,WHEELS_OFF,TAXI_OUT ,DEP_DELAY_NEW, DEP_TIME, CRS_DEP_TIME, \n",
    "DEST_STATE_FIPS, DEST_AIRPORT_ID, DISTANCE, AIR_TIME, \n",
    "   ACTUAL_ELAPSED_TIME, CRS_ELAPSED_TIME, DIVERTED, CANCELLATION_CODE , ARR_TIME_BLK,ARR_TIME,ORIGIN_STATE_FIPS,\n",
    "   ORIGIN_AIRPORT_ID,OP_CARRIER,OP_CARRIER_AIRLINE_ID,OP_UNIQUE_CARRIER,MKT_CARRIER_FL_NUM,MKT_CARRIER,MKT_CARRIER_AIRLINE_ID,\n",
    "   DAY_OF_WEEK\n",
    "   ]\n",
    "\n",
    "#linear regression\n",
    "regressor = tf.contrib.learn.LinearRegressor(feature_columns=linear_features)\n",
    "regressor.fit(input_fn=training_input_fn, steps=10000)\n",
    "regressor.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "\n",
    "#logistic regression\n",
    "logisticregressor = tf.contrib.learn.LinearRegressor(feature_columns=linear_features)\n",
    "logisticregressor.fit(input_fn=training_input_fn, steps=10000)\n",
    "logisticregressor.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "predictionsLinear = list(logisticregressor.predict_scores(input_fn=test_input_fn))\n",
    "print(predictionsLinear)\n",
    "\n",
    "predictionsLinearLarge = list(logisticregressor.predict_scores(input_fn=eval_input_fn))\n",
    "print(predictionsLinearLarge)\n",
    "\n",
    "\n",
    "predictionsLinear = list(regressor.predict_scores(input_fn=test_input_fn))\n",
    "print(predictionsLinear)\n",
    "\n",
    "\n",
    "predictionsLinearLarge = list(regressor.predict_scores(input_fn=eval_input_fn))\n",
    "print(predictionsLinearLarge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('user_environment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e28cc919a1bf074512f85cb4b30369ce78434acb6789f4f069c1d9283cade02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
